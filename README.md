직행(https://zighang.com/) 이라는 채용정보사이트를 활용한 ai관련 직업을 리스트업 하고, 해당 취업이 공시하는 지원자격 및 우대사항을 확인한다.

이를 통해, 해당 직군으로 취업하기 위해 필요한 기술을 확인하고 취업 전략을 수립한다.

`SINGLE_20250826_004620.xlsx`는 listly라는 chrome 확장 프로그램을 사용해 획득한 정보(CompanyName, ApplicationLink, Job Title, ExperienceLevel, EmploymentType, Location)가 포함된 HTMLAttribute를 저장한 sheet이다.

1. 해당 시트에서 정보를 정리/나열한다.
2. 지원자격과 우대사항을 획득 및 추가한다.
3. 해당 정보들을 분석하여 필요로하는 기술을 파악한다.



# 08/26

## 완료 작업
### 1. 직행에서 [회사, 제목, 경력, 근무형태, 학력, 근무지역, 링크]의 형태로 sheet를 만들었다. 여기서 링크는, 직행 내의 공고글의 링크이다.
### 2. 직행 내의 공고의 데이터는 원본 공고의 캡쳐본이기에, 해당 페이지의 '지원하기'버튼을 통해 원본에 접속하고 원본페이지의 주소와 html(지원자격, 우대사항 포함)을 긁어오는 과정을 완성했다.

## 여기서 마주친 한계
### 2번 작업의 과정에서 html을 그대로 gpt-oss모델에 전송하여 지원자격과 우대사항을 정리한 json을 얻으려 하였다(`llm_qual_spec.py`). 
### 이 떄, html의 크기가 한계를 초과 하여, 요구사항을 수행하지 못하였다.
### 완전히 다른 회사들이 소유한 공고이기에, html을 정제하는데에 어려움이 있다.

## 해결 방법
### 일단 기존 sheet에 원문링크를 추가하고, 해당 페이지의 html을 따로 저장한다(4~5개 정도?).
### 기존의 html텍스트(168467)을 4096으로 토큰수를 줄여야 한다......

## 추가적으로
### 'ollama run gpt-oss'로 gpt-oss를 사용할 때와, 'ollama serve'를 통해 사용할 떄 통신방식의 차이를 감안하더라도 속도에서 너무 큰 차이가 난다.
### 이에 같은 로컬 모델이 사용되고 있는 것이 맞는지, 맞다면 왜 이런 차이가 발생하는지 분석할 필요가 있다.



#  9/2

## 완료 작업
### 해당 기업이 요구하는 '자격요건', '우대사항'의 정리
### +@ 전반적인 기업들에서 요구하는 '자격요건', '우대사항'의 획득
### 내가 원하는 대기업을 정하였다. (네이버, 업스테이지, lg)
#### 이유1. ｢독자 AI 파운데이션 모델｣프로젝트의 선발된 기업이다.
#### 이유2. 스크랩한 공고 중 nc와 sk는 맘에 드는 공고가 없었다.
##### 네이버는 회사가 멀어서 조금 그렇다.

## 여기서 마주친 한계
### 로컬로 사용하는(ollama serve) gpt-oss모델이 html을 읽도록 하는게 너무 오래걸린다.
#### `llm_qual_sqpec.ipynb(c7)`에서 약 120여개의 html소스를 읽는데, 95분이 소요되었다.

## 해결방법
### 1. llm모델을 변경한다.
### 2. LLM API 호출 병렬 처리
### 3. 직접 추론으로 변경(e.g., Transformers, llama.cpp 사용)

## 목표
### 동일 작업(120개 HTML 분석) 처리 시간을 10분 이내로 단축.
### 파일을 순차적으로 처리하는 부분을 Python의 multiprocessing 라이브러리를 사용하여 여러 파일을 동시에 처리하는 병렬 방식으로 수정한다.